# -*- coding: utf-8 -*-
"""data_handle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1za52moOXDst0Myg4F4v-GIMbK_AF6nhV
"""

import os
import re
import sys
from itertools import compress
import random
import math
import numpy as np
import rioxarray
import matplotlib
from matplotlib.colors import LogNorm
from matplotlib.ticker import LogFormatter
import matplotlib.pyplot as plt
import xarray
from netCDF4 import num2date
import pandas as pd
import copy
import datetime
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import seaborn as sns

import rasterio
import rasterio.warp
from rasterio.crs import CRS
from rasterio.plot import show
from shapely.geometry import box
import geopy.distance

import io
import imageio
from IPython.display import Image, display
from ipywidgets import widgets, Layout, HBox
import geopandas as gpd
from rasterio.plot import show
from great_circle_calculator.great_circle_calculator import *
from matplotlib_scalebar.scalebar import ScaleBar
from shapely.geometry import box

import adjustText as aT
import cmasher as cmr

"""## Data handle misc"""

def subset_sensor_incatchment(id_code_name, sensors_ds, subset_catchment):
  """
  Extract the sensor in a given catchment
  --- parameters ----

  "id_code_name": str, name of the column containing the identification code
  "sensors_ds": GeoDataFrame, shapefile of sensor location
  "subset_catchment": GeoDataFrame, shapefile of the catchment on which apply the subset

  --- reutrn ---
  list of str
  """
  valid_sensor = []
  for sensor in sensors_ds[id_code_name]:

    sens_within = sensors_ds.loc[sensors_ds[id_code_name] == sensor]
    if ((sens_within.geometry.within(subset_catchment['geometry'][0]).values) ):
      valid_sensor.append(sens_within[id_code_name].values[0])

  return valid_sensor

def max_na_xarrayds(ds, variable, spatial_dim = ("lon", "lat")):
  """
  Find the time step with the maximum number of missing values in a XarrayDataset for
  a given variable. It print the number of missing values (NAs) in that time step, the
  percentage over the number of total pixel, how many time steps there are with the same
  number of NAs, and the index of the first of them

  --- parameters ---
  "ds": xarray.Dataset
  "variable": str, variable to analyze
  "spatial_dim": tuple of str, two other dimension (spatial) over which to sum missing values
  """
  sum_na = ds[variable].isnull().sum(dim=spatial_dim, skipna = True)
  max_na = sum_na.max()
  count_max_na = sum_na.where(sum_na==max_na).count()
  loc_na = sum_na.argmax().values

  return print('Max NAs pixel: '+str(max_na.values) + ' Percentage: '+ str(np.round(max_na.values/24050, 2)* 100)
  + '\nHow many of them: ' + str(count_max_na.values) + " in position: " + str(loc_na))

def inspection_na(ds, variable, time_step, spatial_dim = ("lon", "lat")):
  """
  Retrieve the number of missing values in a given time step for a given variable

  --- parameters ---
  ds": xarray.Dataset
  "variable": str, variable to analyze
  "time_step": int, index of time dimension to inspect for missing values
  "spatial_dim": tuple of str, two other dimension (spatial) over which to sum missing values
  """
  numb_na = ds[variable].isnull().sum(dim=spatial_dim, skipna = True)[time_step].values
  return print("Number of NAs: " + str(numb_na))

def load_ts_in_dictionary(directory, list_name, sep = ";",
                          sensor_identifier = "codice_punto",
                          data_type_specification = {'valore_validato':'float',
                                                     'valore':'float',
                                                     'data':'datetime64[ns]'},
                          value_name = "valore",
                          validated_value_name = "valore_validato",
                          to_eng_decimal_delimiter = True):
  """
  Load the sensor time series csv located in a dictionary into a dictionary
  some specific transformation are needed because of the format of csv adopted
  from ARPA (e.g. decimal delimiter, transformation to float or string etc.)

  --- parameters ---
  "directory": str, directory in which csv are located
  "list_name": list of str, name of the csv file to load
  "sep": str, delimiter in the csv files
  "sensor_identifier": str, column name of the identification code of each sensor
  "data_type_specification": dict, specify the data type to be setted loading the csv
  "value_name", str, column name of measurements
  "validated_value_name", str, column name of validated measurements
  "to_eng_decimal_delimiter": bool, weather to convert from italian to english decimal delimiter

  --- return ---
  dictionary with keys id codes and values the pd.DataFrame of csv
  """

  ds_dict = {}

  for name in list_name:
    ds = pd.read_csv(directory + name,
                       sep = sep, dtype={sensor_identifier:'str'})

    # write English decimal delimiter
    if to_eng_decimal_delimiter is True:
      ds[validated_value_name] = ds[validated_value_name].str.replace(",", ".")
      ds[value_name] = ds[value_name].str.replace(",", ".")

    # transform to float
    ds = ds.astype(data_type_specification)

    # exctract the identification code and use it as the key in the dictionary
    identification_code = ds[sensor_identifier].unique()[0]
    ds_dict[identification_code] = ds

  return ds_dict

"""# Preprocessing - Input Output objects building"""

def select_file_names(directory, filter):
  """
  extract all files in the directory whose name contain the filter
  output a list with names

  --- parameters ---
  "directory": str, directory in which files are located
  "filter": str, query to be executed

  --- return ---
  list of str, values from the query execution
  """
  sel_sensor_names = [os.listdir(directory)[i] for i in range(0,len(os.listdir(directory))) if filter in os.listdir(directory)[i]]
  return sel_sensor_names

def load_ts_in_dictionary(directory, list_name, sep = ";",
                          sensor_identifier = "codice_punto",
                          data_type_specification = {'valore_validato':'float',
                                                     'valore':'float',
                                                     'data':'datetime64[ns]'},
                          value_name = "valore",
                          validated_value_name = "valore_validato",
                          to_eng_decimal_delimiter = True):
  """
  Load the sensor time series csv located in a dictionary into a dictionary
  some specific transformation are needed because of the format of csv adopted
  from ARPA (e.g. decimal delimiter, transformation to float or string etc.)

  --- parameters ---
  "directory": str, directory in which csv are located
  "list_name": list of str, name of the csv file to load
  "sep": str, delimiter in the csv files
  "sensor_identifier": str, column name of the identification code of each sensor
  "data_type_specification": dict, specify the data type to be setted loading the csv
  "value_name", str, column name of measurements
  "validated_value_name", str, column name of validated measurements
  "to_eng_decimal_delimiter": bool, weather to convert from italian to english decimal delimiter

  --- return ---
  dictionary with keys id codes and values the pd.DataFrame of csv
  """

  ds_dict = {}

  for name in list_name:
    ds = pd.read_csv(directory + name,
                       sep = sep, dtype={sensor_identifier:'str'})

    # write English decimal delimiter
    if to_eng_decimal_delimiter is True:
      ds[validated_value_name] = ds[validated_value_name].str.replace(",", ".")
      ds[value_name] = ds[value_name].str.replace(",", ".")

    # transform to float
    ds = ds.astype(data_type_specification)

    # exctract the identification code and use it as the key in the dictionary
    identification_code = ds[sensor_identifier].unique()[0]
    ds_dict[identification_code] = ds

  return ds_dict

"""## Time aggregation"""

def time_aggregation(ds, freq, date_name = "Date", agg_col = "ID_code"):
  """
  aggregate the values in the dataframe 'ds' a the specified frequency 'freq'
  aggregation is performed by averaging

  --- parameters ---
  "ds": pd.DataFrame, dataframe containing a column with np.datetime dates
  "freq": str, frequency to be achieved 'D' for daily, 'W' for weekly
  "date_name": str, column name of dates
  "agg_col": str, reference column for computing aggregation

  --- return ---
  pd.DataFrame aggregated in time
  """

  ds_daily = ds.set_index(date_name).groupby([pd.Grouper(freq=freq), agg_col]).mean()
  ds_daily=ds_daily.reset_index(drop = False)

  return ds_daily

"""## Dealyed input creation"""

def insert_missing_dates(ds, date_name, temp_freq):

  """
  Create instance with NA for missing dates in a pd.Dataframe

  --- parameters ---
  "ds": pd.DataFrame, dataframe containing a column with np.datetime dates
  "date_name": str, column name of dates
  "temp_freq": str, temporal frequency of the dataset

  --- return ---
  pd.DataFrame with NA inserted in missing dates
  """

  ds_out = copy.deepcopy(ds)
  present_date = ds_out[date_name].values
  # compute all the dates between the maximum and minimum present dates
  range_date = pd.date_range(present_date.min(), present_date.max(), normalize = True, freq=temp_freq, inclusive = "both")
  # compute missing dates
  missing_dates = range_date[[range_date[i] not in present_date for i in range(0,len(range_date))]]

  # add rows in the dataset with missing dates - NAs will be inserted
  ds_out = pd.concat([ds_out,pd.DataFrame(missing_dates, columns=[date_name])])

  # sort the dataset by date
  ds_out.sort_values(by=date_name, inplace = True)
  ds_out = ds_out.reset_index(drop=True)

  return ds_out

def compute_delta_column(ds, delta_t_name = "Delta_time", temp_freq = "D",
                         date_name = "Date", id_name = "ID_code", value_name = "Value"):

    """
    Add a new column named 'delta_t_name' containing the time distances between
    the the current row and the subsequent.

    --- parameters ---
    "ds": pd.DataFrame, input dataset in which delta time between consecutive rows have to be computed
    "delta_t_name": str, name to be given to the new delta column
    "temp_freq": str, temporal frequency of the dataset
    "date_name": str, name of the column storing the dates
    "id_name": str, name of the column storing the identification code of the sensor
    "value_name": str, name of the column storing the values of the time series

    --- return ---
    pd.DataFrame with a new column containing time distances
    """

    delta_freq = "timedelta64[" + temp_freq + "]"

    delta_value = ds.loc[1:,date_name].reset_index(drop = True) - ds.loc[:ds.index[-2],date_name].reset_index(drop = True)
    # -2 because .loc include both a and b in a:b

    delta_value = delta_value.astype(delta_freq)
    delta_value = delta_value.astype("float")

    ds = ds.drop(ds.shape[0]-1) # remove first row after differentiation
    ds[delta_t_name] = delta_value
    ds = ds.loc[:,[date_name, id_name, delta_t_name, value_name]]

    return ds

def normalize_delta(ds, delta_var,
                    offset_norm = 10**(-4), output_cache = False,
                    min_delta = None, max_delta = None):

  """
  Min-Max normalization of columns of a dataset - intended for normalizing
  delta values but it normalize each column specified in delta_var. Be aware that
  it consider all the columns to be jointly normalized, i.e. min and max are intended
  for all the values of all the column and not columns wide (this is not a column
  wise normalization).

  --- parameters ---
  "ds": pd.DataFrame, input dataset
  "delta_var": str or list of str, contains coulmn names
  "output_cache": bool. weather to output normalization factors
  "min_delta","max_delta": float, optionals, normalization factors if not provided they are automatically computed
  among all the columns

  --- return ---

  pd.DataFrame with a multiple columns min-max normalization
  """

  ds_norm = copy.deepcopy(ds)


  if (min_delta == None or max_delta == None):
    min_delta = ds_norm[delta_var].min()
    max_delta = ds_norm[delta_var].max()

  ds_norm[delta_var] = ((ds_norm[delta_var] - min_delta)/(max_delta - min_delta)) + offset_norm



  if (output_cache == True):
    output_list = [ds_norm, min_delta, max_delta]
    return tuple(output_list)

  else:
    return ds_norm

def variable_time_shift(ds, variable, shift, na_fill = False,
                        shift_suffix = "_t", return_name = True):
    """
    compute a shifted version of a "variable" (a column) and insert it in a copy
    of the dataset (ds)
    name of the new column: "variable"+"shift_suffix"+shift

    --- parameters ---
    "ds": pd.DataFrame, input dataset
    "variable": str, column to be shifted
    "na_fill": bool, if true the new column will be filled with np.nan values this is useful only for
    column initialization purposes
    "shift_suffix": str, name to be appended to the original column name
    "return_name": bool, True for retrieving the new column name

    --- return ---
    tuple(pd.DataFrame, str)
    """
    ds_shifted = copy.deepcopy(ds)

    if (shift>0):
      shift_suffix += "-"
    else:
      shift_suffix += "+"

    delayed_variable_name = variable + shift_suffix + str(abs(shift))

    if (na_fill == False):
      ds_shifted[delayed_variable_name] = ds_shifted[variable].shift(shift)
    else:
      ds_shifted[delayed_variable_name] = np.nan

    output_list = [ds_shifted]
    if (return_name == True):
      output_list.extend([delayed_variable_name])

    return tuple(output_list)

def delayed_ds(ds, variable_to_shift, shift_sequence,
               na_fill = False, time_ordering = True):
  """
  compute a delayed version of a dataset shifting multiple times all
  the "variable_to_shift"

  --- parameters ---
  "ds": pd.DataFrame, input dataset
  "variable_to_shift": list of str, columns to be shifted
  "shift_sequence": iterable, shifts to be computed, shifts are sorted in the same order of the element inside the list.
                    Shift could be positive (a delay) or negative (ahead shift)
  "na_fill": bool, if true the new column will be filled with np.nan values
  "time_ordering": bool, if True shift all the variable per each time shift then increase the shift (i.e. first all t-i for all the variable then t-(i+1))

  --- return ---
  tuple(pd.DataFrame with, list of str) shifted dataframe and list of names of shifted columns
  """

  ds_shifted = copy.deepcopy(ds)

  shifted_variables = []

  if (time_ordering == True):
    # outer loop over time
    shift_index = 0
    # inner loop over variables
    variable_index = 1

    outer_range = shift_sequence
    inner_range = variable_to_shift

  else:
    # inner loop over variables
    shift_index = 1
    # outer loop over time
    variable_index = 0

    outer_range = variable_to_shift
    inner_range = shift_sequence


  for i in outer_range:

    for j in inner_range:

      indexes = [i,j]
      ds_shifted, shifted_name = variable_time_shift(ds = ds_shifted, variable = indexes[variable_index],
                                                        shift = indexes[shift_index], na_fill = na_fill,
                                                        return_name = True)

      shifted_variables.append(shifted_name)

  return ds_shifted, shifted_variables

def find_value_from_queries_among_keys(queries, keys):
  """
  given a set of queries extract each key which contains any of the queries
  used for extract all names of delayed features or deltas (eg. delta-1,..., delta-p)
  "queries" must be a list, even if it is only a single string

  --- parameters ---
  "queries": list of str, the queries
  "keys": list of str, the keys

  --- return ---
  list of str with values
  """
  boolean = [any(query in key for query in queries) for key in keys]
  values = [keys[i] for i in range(0,len(keys)) if boolean[i]]
  return values

"""## Splitting"""

# for splitting datasets into training, validation, and test

def next_row_after_margin(ds, date_name, last_set_observation,
                          split_margin, temp_freq = None, count_time = False):

      """
      Used in "train_test_split_ds" function. It find the first valid row number
      of the next (second) split given a margin (gap) to be respected
      It is needed to make two consecutive splits independent.

      --- parameters ---
      "ds": pd.DataFrame, input dataset before the split
      "date_name": str, name of the column storing the dates
      "last_set_observation": int, row number of the last obersvation in the first set (computed in the "ds")
      "split_margin": int, a margin to be left between the two splits
      "count_time": bool, if True the "split_margin" is intended in time units defined by "temp_freq", otherwise as rows number
      "temp_freq": str, temporal frequency of the "ds"

      --- return ---
      int, row number of the first valid observation of the second set. Row number computed in the "ds"
      """

      if (count_time == True):
          first_valid_date = ds[date_name].iloc[last_set_observation] + np.timedelta64(split_margin, temp_freq)

          first_valid_index = ds[date_name][(ds[date_name]>first_valid_date)].idxmin()
          # retrive row number of the first valid date
          first_valid_position = ds[date_name].index.get_loc(first_valid_index)

      else:
          first_valid_position = last_set_observation + split_margin + 1

      return first_valid_position

def next_row_before_margin(ds, date_name, first_date_next_split,
                           split_margin, first_set_observation = None,
                           temp_freq = None, count_time = False):

      """
      Used in "train_test_split_ds" function. It find the last valid row number
      of the current (first) split given a margin (gap) to be respected
      It is needed for make the two/more splits independent.

      --- parameters ---
      "ds": pd.DataFrame, input dataset before the split
      "date_name": str, name of the column storing the dates
      "first_set_observation": int, row number of first obervation in the second split (computed in the "ds")
      "split_margin": int, a margin to be left between the two splits
      "count_time": bool, if True the "split_margin" is intended in time units defined by "temp_freq", otherwise as rows number
      "temp_freq": str, temporal frequency of the "ds"

      --- return ---
      int, row number of the last valid observation of the first set. Row number computed in the "ds"
      """

      if (count_time == True):
          last_valid_date = first_date_next_split - np.timedelta64(split_margin, temp_freq)

          last_valid_index = ds[date_name][(ds[date_name]<last_valid_date)].idxmax()
          # retrive row number of the first valid date
          last_valid_position = ds[date_name].index.get_loc(last_valid_index)

      else:
          last_valid_position = first_set_observation - split_margin

      return last_valid_position



def train_test_split_ds(ds, date_name, temp_freq, training_perc = None, valid_perc = None,
                        only_last_as_test = False,
                        training_upto_date = None,
                        test_from_date = None,
                        step_ahead = 0, delays = 0,
                        retrive_limits_info = False,
                        split_margin = None, count_time = False):
  """
  Split a dataset ("ds") in training, validation (optionally), and test. It is possible to
  specify a margin between sets to preserve independence and avoid leakage.

  in the case "test_from_date" is provided, first the test set is defined introducing a gap
  on the previous remaining data, then the training and validation are computed introducing a gap
  on the validation observations.

  in the case of sets defined by percentage the gaps are introduced in the validation and test sets.

  --- parameters ---
  "ds": pd.DataFrame, input dataset before the split it should be well indexed (no holes) and without np.nan/NaT/etc
  "temp_freq": str, temporal frequency of the "ds"
  "training_perc": float, optional, if specified defined the percentage of training observations
  "valid_perc": float, optional, if specified defined the percentage of validation observation if == 0 no validation set will be created
  "only_last_as_test": bool, whether to consider as test set the last instance of "ds" in this case no validation set is constructed
  "training_upto_date": np.datetime, optional, if provided the training set is created with all the observation up to that date
  "test_from_date": np.datetime, optional, if provided the test set is create with all the observation from that date
  "step_ahead": int, if 0 is a seq2val scanario if > 0 IS seq2seq and in this case it automatically detect the "split_margin" to be used
  "delays": int, number of delays of the target variable used as features. It is needed to detect automatically the "split_margin" to be used
  "retrive_limits_info": bool, whether to output also a list with the rows number of sets limits
  "split_margin": int, a margin to be left between consecutive splits. If not provided it is set equal to max("delays","step_ahead")
  "count_time": bool, if True the "split_margin" between the two dataset is intended in time units defined by "temp_freq", otherwise as a rows number

  --- return ---

  tuple with training, validation (if requested), and test set, set_limits (if required)
  sets are given as pd.DataFrame, set_limits is a list
  """

  if (split_margin == None):
      split_margin = max(delays,step_ahead)
      # the minimum to make splitted dataset independent

  else:
      if (split_margin < max(delays,step_ahead)):
        # if a split_margin is provided it should be at least equal to "delays"
        warnings.warn("split_margin is lower the maximum between the number of delays and step_ahead! Forced to be equal ...\n")
        split_margin = max(delays,step_ahead)


  if (only_last_as_test==True):
        ################################################
        # TEST SET DEFINED BY LAST SET AHEAD OBSERVATIONS
        ################################################
        assert(test_from_date == None), "test_from_date and only_last_as_test both specified..."
        # in this case all data are training obs but the last step_ahead time steps
        training_tot_obs = ds.shape[0]-(step_ahead+1)
        last_training_index = training_tot_obs - 1
        training_ds = ds.iloc[:last_training_index+1,:]

        # retrieve training set limit
        set_limits  = [last_training_index]
        output_list = [training_ds]

        # build the test set with only the last step_ahead time steps
        # this is a use-case-scenario: no independence issue between training and test sets!
        first_test_index = last_training_index+1
        test_ds = ds.iloc[first_test_index:,:]

        # output list and return
        output_list.append(test_ds)
        if (retrive_limits_info == True):
        # in case limits of the splitted datasets are requested as additional output
          output_list.extend(set_limits)

        return tuple(output_list)


  if (test_from_date != None):
        ####################################
        # TEST SET DEFINED BY A SPECIFIC DATE
        ####################################
        test_ds = ds.loc[ds[date_name]>=test_from_date]
        first_test_date = test_ds[date_name].min()

        # dependency constraint: leave a gap between sets
        last_index = next_row_before_margin(ds, date_name,
                                first_date_next_split = first_test_date,
                                split_margin = split_margin,
                                temp_freq = temp_freq,
                                count_time = count_time)

        ds = ds.iloc[:last_index+1,:]

        if(valid_perc!=0):

            # if a validation set will be present the independece issue will be
            # imposed on the validation set
            if training_upto_date is None:
              training_tot_obs = math.ceil((ds.shape[0])*training_perc)
              last_training_index = training_tot_obs - 1
            else:
              last_training_index = ds[date_name][(ds[date_name]<=training_upto_date)].idxmax()
              last_training_index = ds[date_name].index.get_loc(last_training_index)


            training_ds = ds.iloc[:last_training_index+1,:]

            set_limits  = [last_training_index]
            output_list = [training_ds]


            # find the first row of the validation set after the split_margin

            first_val_index = next_row_after_margin(ds = ds, date_name = date_name,
                                            last_set_observation = last_training_index,
                                            split_margin = split_margin,
                                            temp_freq = temp_freq,
                                            count_time = count_time)



            validation_ds = ds.iloc[first_val_index:,:]

            set_limits.append(len(ds)-1)
            output_list.append(validation_ds)

        else:
            # no validation set
            training_ds = ds
            set_limits  = [len(ds)-1]
            output_list = [training_ds]




  else:
        ###################################
        # TEST SET SPECIFIED BY A PERCENTAGE
        ###################################

        training_tot_obs = math.ceil((ds.shape[0])*training_perc)
        last_training_index = training_tot_obs - 1
        training_ds = ds.iloc[:last_training_index+1,:]

        set_limits  = [last_training_index]
        output_list = [training_ds]

        if(valid_perc!=0):

            # if a validation set will be present the independece issue will be
            # imposed on the validation set


            # find the first row of the validation set after the split_margin

            first_val_index = next_row_after_margin(ds = ds, date_name = date_name,
                                          last_set_observation = last_training_index,
                                          split_margin = split_margin,
                                          temp_freq = temp_freq,
                                          count_time = count_time)

            valid_tot_obs = math.ceil((ds.shape[0])*valid_perc)

            last_val_index = first_val_index + valid_tot_obs - 1

            validation_ds = ds.iloc[first_val_index:last_val_index+1,:]

            set_limits.append(last_val_index)
            output_list.append(validation_ds)


        # in that case of test set specified by a percentage,
        # the dependency issue should be imposed also the test set
        # either if a validations set is requested or not

        first_test_index = next_row_after_margin(ds = ds, date_name = date_name,
                                            last_set_observation = set_limits[-1], # - step_ahead,
                                            split_margin = split_margin,
                                            temp_freq = temp_freq,
                                            count_time = count_time)

        test_ds = ds.iloc[first_test_index:,:]





  # output definition valid either the test is specified by a percentage or by a specific date
  output_list.append(test_ds)
  if (retrive_limits_info == True):
  # in case limits of the splitted datasets are requested as additional output
    output_list.extend(set_limits)

  return tuple(output_list)

def filter_ds_by_coulmn(ds, selected_columns, numpy_out = False):
  """
  Filter a DataFrame by columns

  --- parameters ---
  "ds": pd.DataFrame, input dataset
  "selected_columns":str or list of str, column names
  "numpy_out": bool, if false output a pandas df otherwise a numpy array

  --- return ---
  filtered dataframe
  """

  ds_filtered = copy.deepcopy(ds)
  ds_filtered = ds_filtered.loc[:,selected_columns]

  if (numpy_out == True):
    ds_filtered = ds_filtered.values

  return ds_filtered

def from_pandas_to_numpy(ds_list):
  """
  transform each pandas dataframe in the list "ds_list" into a numpy array
  """
  output_list = [one_ds.values for one_ds in ds_list]
  return output_list

"""## Meteo"""

def create_x_meteo_obj(meteo_ds, target_dates, time_coord = "time",
                         numb_past_images = 104, temp_freq = 'W',
                         channel = "variables",
                         month_OneHotEnc = False,
                         encoder = None):

  """
  Construct the input object (X) made of a time series of meteorological images of length 'numb_past_images'. The time series of images could be arranged
  in two different way depending on the type of models. In the case of time distributed models, the dimension should be (batch, time, width, height, variable) then the
  channel dimension is represented by variable. In the case of variable distributed models instead the dimension should be (batch, variable, width, height, time), then the channel is represented by time

  For the article only time distributed models are developed then the function is used every time with channl = "variables" and
  it creates a list with 3 elements:
    1) A numpy-array (samples, past_meteo_images, height, width, variables) with the image time series for each sample
    2) A list with "samples" element, each element contains the dates of each image in the image time series of the corresponding sample
    3) A numpy-array (samles, past_meteo_images, 11) contains the one hot encoding of the months in each image time series


  --- parameters ---
  "meteo_ds": xarray.Dataset, dataset with dimensions: time, x (long), y (lat), and weather variables
  "target_dates": list of np.datetime, contains the dates of the target to be predicted
  "time_coord": str, name of the coordinate representing time
  "numb_past_images": int, length of the image time series to be constructed
  "temp_freq": str, temporal frequency of the image time series to be considered
  "channel" str, what to insert in the channel dimension
  "month_OneHotEnc": bool, whether to compute the OHE for each time step of the image time series
  "encoder": sklearn.preprocessing.OneHotEncoder, required if "month_OneHotEnc" is True

  --- return ---

  list with three element [np.array, list, np.array]

  """

  if (channel == "time"):
      variables = list(meteo_ds.keys())
      x_dictionary = {variables[i] : [[],[]] for i in range(0,len(variables))}

      for i in range(0,len(target_dates)):
          for var in variables:
            meteo_values = meteo_ds[var].loc[(target_dates[i]-np.timedelta64(numb_past_images,temp_freq)):target_dates[i]-np.timedelta64(1,temp_freq)].values
            meteo_values = np.moveaxis(meteo_values, 0, -1)
            meteo_dates = meteo_ds[var].loc[(target_dates[i]-np.timedelta64(numb_past_images,temp_freq)):target_dates[i]-np.timedelta64(1,temp_freq)][time_coord].values
            x_dictionary[var][0].append(meteo_values)
            x_dictionary[var][1].append(meteo_dates)
            print('\r' + str(round(i/(len(target_dates)-1), 4) * 100) + ' %. Image in [0] position with shape: ' + str(meteo_values.shape) + ' - Total processed observations: ' + str(len(x_dictionary[var][0])), end = '')

      for var in variables:
        x_dictionary[var][0] = np.stack(x_dictionary[var][0], axis=0)

      return x_dictionary

  elif (channel == "variables"):


      image_list = []
      date_list = []
      if (month_OneHotEnc is True):
        month_list = []


      for i in range(0,len(target_dates)):

            end_date = target_dates[i]#-np.timedelta64(1,temp_freq) because np.arange does not include last value
            start_date = target_dates[i]-np.timedelta64(numb_past_images,temp_freq)
            date_sequence = np.arange(start_date, end_date, np.timedelta64(1, temp_freq))

            #meteo_values = meteo_ds.sel(time=meteo_ds.time.isin(date_sequence)) #meteo_dataset_rain_snow_weekly.time.isin(["lucifer", "judas"])
            meteo_values = meteo_ds.where(meteo_ds[time_coord].isin(date_sequence), drop=True).to_array().values
            # dimension order should be (time, y (height), x (width), variable)
            meteo_values = np.moveaxis(meteo_values, 0, -1)

            #### if One Hot Encoding of months should be reported
            if (month_OneHotEnc is True):
              month_array = date_sequence.astype('datetime64[M]').astype(int) % 12 + 1
              month_array = month_array.astype('str')
              month_array = [[month_array[i]] for i in range(len(month_array))]
              month_encodings = encoder.transform(month_array)
              month_list.append(month_encodings)


            image_list.append(meteo_values)
            date_list.append(date_sequence)

            print('\r' + str(round((i+1)/(len(target_dates)),4) * 100) + ' %. Image in [0] position with shape: ' + str(meteo_values.shape) + ' - Total processed observations: ' + str(len(image_list)), end = '')


      image_list = np.stack(image_list, axis=0)
      print("")

      if (month_OneHotEnc is True):
        x_list = [image_list, date_list, np.array(month_list)]
      else:
        x_list = [image_list, date_list]

      return x_list

  else:
    return print("Unrecognised channel specification ")

"""# Saving"""

def save_values_from_dict(ds_dict, keys_list, sensors_path,
                               value_type = "pandas",
                               list_element = None):
  """
  Save all the dataset in the dictionary in separate csv files
  the keys of the dictionary should be in the list keys_list
  sensors_path define the path in which to save the files

  --- parameters ---
  "ds_dict": dict, dictionary with values object to be save and key key_list
  "keys_list": list of str, list of dictionary's key
  "sensors_path": list of str, contains the direcories for each value to be saved
  "value_type": str, define the object type in values. Available type: "pandas" pd.DataFrame saved as csv, "numpy" np.array saved as .npy, "feature_list" list saved as .npy, "xarray" xarray.Dataset saved as .nc
  "list_element": int, optional, index position of the element in the list to be saved

  --- return ---
  None save the objects
  """

  for i in range(len(keys_list)):

      value_object = ds_dict[keys_list[i]]

      if (value_type == "pandas"):
        value_object.to_csv(sensors_path[i], index = False)
      elif (value_type == "numpy"):
        np.save(sensors_path[i], value_object)
      elif (value_type == "feature_list"):
        np.save(sensors_path[i], value_object[list_element])
      elif (value_type == "xarray"):
        value_object.to_netcdf(sensors_path[i])

      print(keys_list[i] + " path: " + sensors_path[i])

  return None