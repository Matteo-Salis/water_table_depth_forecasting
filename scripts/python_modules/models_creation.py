# -*- coding: utf-8 -*-
"""models_creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u2l7TCm3rCDzpE2UpEulkwzcm0BFqEyd
"""

import numpy as np
import xarray
import copy
import datetime
from datetime import datetime, timedelta
import datetime
import os
from itertools import compress
import random
import re
import math

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import activations
import sys
from functools import partial

def meteo_time_distributed_conv_module(meteo_input,
                                       kernel_regularizer, bias_regularizer,
                                       batch_normalization = False,
                                       bn_epsilon = 0.001,
                                       kernel_initializer = "HeUniform"):

    """
    Function to define the Time Distributed CNN (TDC) module

    --- parameters ---
    "meteo_input": array-like KerasTensor, input of dimension (batch, time, height, width, channels)
    "kernel_regularizer": tf.keras.regularizers, a kernel regularizer to be used in each layer it could be None
    "bias_regularizer": tf.keras.regularizers, a bias regularizer to be used in each layer it could be None
    "batch_normalization": bool, wether to use Batch Normalization, if yes it is placed after convolution and before activation as in ResNet
    "bn_epsilon": float, the epsilon parameter to be used in the BN
    "kernel_initializer": str, initializer for kernel weights

    --- return ---
    KerasTensor
    """

    if (batch_normalization is True):
      batch_norm = partial(tf.keras.layers.BatchNormalization,
                  epsilon=bn_epsilon )

    conv2d_16 = layers.Conv2D(8, kernel_size = (2,2), strides = 1, padding = "same", activation=None,
                              kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                              kernel_initializer=kernel_initializer)
    conv2d_time_distributed = layers.TimeDistributed(conv2d_16)(meteo_input)
    if (batch_normalization is True):
      conv2d_time_distributed = batch_norm()(conv2d_time_distributed)
    conv2d_time_distributed = tf.keras.layers.LeakyReLU()(conv2d_time_distributed)

    conv2d_32_0 = layers.Conv2D(16, kernel_size = (2,2), strides = 1, padding = "same", activation=None,
                                kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                                kernel_initializer=kernel_initializer)
    conv2d_time_distributed = layers.TimeDistributed(conv2d_32_0)(conv2d_time_distributed)
    if (batch_normalization is True):
      conv2d_time_distributed = batch_norm()(conv2d_time_distributed)
    conv2d_time_distributed = tf.keras.layers.LeakyReLU()(conv2d_time_distributed)

    conv2d_32_1 = layers.Conv2D(16, kernel_size = (2,2), strides = 2, padding = "same", activation=None,
                                kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                                kernel_initializer=kernel_initializer)
    conv2d_time_distributed = layers.TimeDistributed(conv2d_32_1)(conv2d_time_distributed)
    if (batch_normalization is True):
      conv2d_time_distributed = batch_norm()(conv2d_time_distributed)
    conv2d_time_distributed = tf.keras.layers.LeakyReLU()(conv2d_time_distributed)

    conv2d_32_4 = layers.Conv2D(16, kernel_size = (2,2), strides = 2, padding = "same", activation=None,
                                kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                                kernel_initializer=kernel_initializer)
    conv2d_time_distributed = layers.TimeDistributed(conv2d_32_4)(conv2d_time_distributed)
    if (batch_normalization is True):
      conv2d_time_distributed = batch_norm()(conv2d_time_distributed)
    conv2d_time_distributed = tf.keras.layers.LeakyReLU()(conv2d_time_distributed)

    average_pooling_1 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides = 1, padding="valid")
    conv2d_time_distributed = layers.TimeDistributed(average_pooling_1)(conv2d_time_distributed)

    flatten = layers.Flatten()
    conv2d_time_distributed = layers.TimeDistributed(flatten)(conv2d_time_distributed)

    return conv2d_time_distributed

### UnPWaveNet layer ##########

def wavenet_layer_no_padding(input, layer_number, dilation,
                             channels_dilated_conv, channels_1x1conv,
                             kernel_regularizer_dilated_conv, kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv, bias_regularizer_1x1_conv,
                             kernel_initializer,
                             batch_normalization = False,
                             bn_epsilon = 0.001,
                             gated_activation = False,
                             kernel_dilated_conv = 4,
                             residual_input = None):

  """
  A wavenet layer without causal padding

  --- parameters ---
  "input": array-like KerasTensor, layer input
  "layer_number": int, ordering number of the layer to be created
  "dilation": int, dilation factor to be used in the convolution
  "channels_dilated_conv": number of filter to be used in the dilated convolution
  "kernel_dilated_conv": int, size of the kernel of the dilated convolution
  "channels_1x1conv": number of filter to be used in the 1x1 convolution
  "kernel_regularizer_dilated_conv": tf.keras.regularizers, kernel regularizer for the dilated convolution
  "kernel_regularizer_1x1_conv": tf.keras.regularizers, kernel regularizer for the 1x1 convolution
  "bias_regularizer_dilated_conv":tf.keras.regularizers, bias regularizer for the dilated convolution
  "bias_regularizer_1x1_conv": tf.keras.regularizers, bias regularizer for the 1x1 convolution
  "kernel_initializer": str, initializer for kernel weights
  "batch_normalization": bool, wether to use Batch Normalization, if yes it is placed after convolution and before activation as in ResNet
  "bn_epsilon": float, the epsilon parameter to be used in the BN
  "gated_activation": bool, whether to use gated activation or not if True sigmoid and leaky-relu are used
  "residual_input": array-like KerasTensor, needed for the first layer in which the residual connection is provided externally

  --- return ---
  KerasTensor

  """

  if (batch_normalization is True):
      batch_norm = partial(tf.keras.layers.BatchNormalization,
                  epsilon=bn_epsilon )

  dilated_conv_value =  layers.Conv1D(channels_dilated_conv, kernel_size = kernel_dilated_conv, dilation_rate = dilation,
                                                        padding = "valid", activation = "leaky_relu",
                                                        kernel_regularizer = kernel_regularizer_dilated_conv,
                                                        bias_regularizer=bias_regularizer_dilated_conv,
                                                        kernel_initializer=kernel_initializer,
                                                        name = f"dilated{dilation}_conv_value_Wlayer{layer_number}")(input)

  if (gated_activation is True):
      #########
      dilated_conv_weights =  layers.Conv1D(channels_dilated_conv, kernel_size = kernel_dilated_conv, dilation_rate = dilation,
                                                          padding = "valid", activation = "sigmoid",
                                                          kernel_regularizer = kernel_regularizer_dilated_conv,
                                                          bias_regularizer=bias_regularizer_dilated_conv,
                                                          kernel_initializer="glorot_uniform",
                                                          name = f"dilated{dilation}_conv_weights_Wlayer{layer_number}")(input)


      dilated_conv_value = tf.math.multiply(dilated_conv_value, dilated_conv_weights)
      ##########

  if (batch_normalization is True):
      dilated_conv_value = batch_norm()(dilated_conv_value)
  dilated_conv_value = tf.keras.layers.LeakyReLU()(dilated_conv_value)


  conv_1x1 =  layers.Conv1D(channels_1x1conv, kernel_size = 1, activation = None,
                                                        kernel_regularizer = kernel_regularizer_1x1_conv,
                                                        bias_regularizer = bias_regularizer_1x1_conv,
                                                        kernel_initializer = kernel_initializer,
                                                        name = f"conv_1x1_Wlayer{layer_number}")(dilated_conv_value)

  if (batch_normalization is True):
      conv_1x1 = batch_norm()(conv_1x1)

  if (residual_input is None):
    residual_pooling_kernel = kernel_dilated_conv+(dilation-1)*(kernel_dilated_conv-1) # to match dimensions
    residual_input = layers.AveragePooling1D(pool_size=residual_pooling_kernel,
                                             strides = 1,
                                             name = f"pool_avg{residual_pooling_kernel}_Wlayer{layer_number}")(input)

  residual_add = layers.Add()([conv_1x1, residual_input])
  residual_add = tf.keras.layers.LeakyReLU()(residual_add)

  layer_output = [conv_1x1, residual_add]

  return tuple(layer_output)


def meteo_model_time_dist_dilated_unpwavenet(meteo_input = None, output_neurons = None,
                        past_meteorological_images = 104,
                        input_month_encodings = True,
                        dropout = None,
                        perc_dropout = 0.15,
                        perc_dropout_unpwavenet = 0,
                        channels_dilated_conv = 32,
                        kernel_dilated_conv = 4,
                        channels_1x1conv = 8,
                        gated_activation = True,
                        kernel_regularizer = None,
                        kernel_regularizer_dilated_conv = None,
                        kernel_regularizer_1x1_conv = None,
                        bias_regularizer = None,
                        bias_regularizer_dilated_conv = None,
                        bias_regularizer_1x1_conv = None,
                        kernel_initializer = "HeUniform",
                        batch_normalization = False,
                        bn_epsilon = 0.001):
  """
  Initialization of the TDC-UnPWaveNet architecture

  --- parameters ---
  "meteo_input": array-like KerasTensor, optional, input data
  "output_neurons": int, number of output neurons. Current version implemented for the seq2val case (output_neurons = 1)
  "past_meteorological_images": int, time-length of the input image time series
  "input_month_encodings": whether to use as input also the OHE of each time step of the input image time series
  "dropout": str, optional, whether to use dropout between the TDC and the UnPWaveNet. it could be "classic" or "mc" for the monte carlo dropout
  "perc_dropout": float, dropout probability
  "perc_dropout_unpwavenet": float, optional, if provided insert a dropout layer after the concatenation of the skip connection
  "channels_dilated_conv": number of filter to be used in the dilated convolution
  "kernel_dilated_conv": int, size of the kernel of the dilated convolution
  "channels_1x1conv": number of filter to be used in the 1x1 convolution
  "kernel_regularizer": tf.keras.regularizers, kernel regularizer for the TDC module
  "kernel_regularizer_dilated_conv": tf.keras.regularizers, kernel regularizer for the dilated convolution
  "kernel_regularizer_1x1_conv": tf.keras.regularizers, kernel regularizer for the 1x1 convolution
  "bias_regularizer": tf.keras.regularizers, bias regularizer for the TDC module
  "bias_regularizer_dilated_conv": tf.keras.regularizers, bias regularizer for the dilated convolution
  "bias_regularizer_1x1_conv": tf.keras.regularizers, bias regularizer for the 1x1 convolution
  "kernel_initializer": str, initializer for kernel weights
  "batch_normalization": bool, wether to use Batch Normalization, if yes it is placed after convolution and before activation as in ResNet
  "bn_epsilon": float, the epsilon parameter to be used in the BN
  "gated_activation": bool, whether to use gated activation or not if True sigmoid and leaky-relu are used

  --- return ---
  a Keras.Model

  """

  if(meteo_input is None):
      # In a KerasTensor is not provided
      meteo_input = layers.Input(shape=(past_meteorological_images,5,8,3))


  if (dropout == "mc"):
    # MC dropout: drop either in training and inference
    class doprout_custom(tf.keras.layers.SpatialDropout1D):
      def call(self, inputs, training):
        return super().call(inputs, training=True)

  if (dropout == "classic"):
    class doprout_custom(tf.keras.layers.SpatialDropout1D):
      def call(self, inputs, training):
        return super().call(inputs, training)

  meteo_flattened = meteo_time_distributed_conv_module(meteo_input,
                                                       kernel_regularizer = kernel_regularizer,
                                                       bias_regularizer = bias_regularizer,
                                                       batch_normalization = batch_normalization,
                                                       bn_epsilon = bn_epsilon,
                                                       kernel_initializer=kernel_initializer)


  if (batch_normalization is True):
      batch_norm = partial(tf.keras.layers.BatchNormalization,
                  epsilon=bn_epsilon)

  if (input_month_encodings is True):
      month_encodings = layers.Input(shape=(past_meteorological_images,11))
      model_input = [meteo_input, month_encodings]

      meteo_flattened = keras.layers.Concatenate(axis=-1)([meteo_flattened, month_encodings])

  else:
      model_input = meteo_input


  input_bottleneck = layers.Conv1D(16, kernel_size = 1, dilation_rate = 1,
                                   activation = None,
                                   kernel_regularizer = kernel_regularizer,
                                   bias_regularizer=bias_regularizer,
                                   kernel_initializer=kernel_initializer)(meteo_flattened)

  if (batch_normalization is True):
      input_bottleneck = batch_norm()(input_bottleneck)
  input_bottleneck = tf.keras.layers.LeakyReLU()(input_bottleneck)


  if (dropout is not None and perc_dropout > 0):
      # apply a dropout this should help the network to use all the channels and
      # avoid that one channel overifits
      input_bottleneck = doprout_custom(perc_dropout)(input_bottleneck)


###### UnPWaveNet ##########################
  # residual connection 0

  decoder_residual_0 = layers.Conv1D(channels_1x1conv, kernel_size = 1, dilation_rate = 1,
                                                        padding = "same", activation = None,
                                                        kernel_regularizer = kernel_regularizer_1x1_conv,
                                                        bias_regularizer=bias_regularizer_1x1_conv,
                                                        kernel_initializer=kernel_initializer)(input_bottleneck)

  if (batch_normalization is True):
      decoder_residual_0 = batch_norm()(decoder_residual_0)

  pooling_res_0 = layers.AveragePooling1D(pool_size=4, strides = 1)(decoder_residual_0)

  # -------------------- #
  # Layer 1: dilation 1
  # -------------------- #
  wavenet_no_padding_layer_1_skip, wavenet_no_padding_layer_1_output  = wavenet_layer_no_padding(input = input_bottleneck,
                             layer_number = 1, dilation = 1,
                             channels_dilated_conv = channels_dilated_conv,
                             channels_1x1conv = channels_1x1conv,
                             kernel_regularizer_dilated_conv = kernel_regularizer_dilated_conv,
                             kernel_regularizer_1x1_conv = kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv = bias_regularizer_dilated_conv,
                             bias_regularizer_1x1_conv = bias_regularizer_1x1_conv,
                             kernel_initializer = kernel_initializer,
                             gated_activation = gated_activation,
                             kernel_dilated_conv = kernel_dilated_conv,
                             residual_input = pooling_res_0,
                             batch_normalization = batch_normalization,
                             bn_epsilon = bn_epsilon)

  # -------------------- #
  # Layer 2: dilation 2
  # -------------------- #
  wavenet_no_padding_layer_2_skip, wavenet_no_padding_layer_2_output  = wavenet_layer_no_padding(input = wavenet_no_padding_layer_1_output,
                             layer_number = 2, dilation = 2,
                             channels_dilated_conv = channels_dilated_conv,
                             channels_1x1conv = channels_1x1conv,
                             kernel_regularizer_dilated_conv = kernel_regularizer_dilated_conv,
                             kernel_regularizer_1x1_conv = kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv = bias_regularizer_dilated_conv,
                             bias_regularizer_1x1_conv = bias_regularizer_1x1_conv,
                             kernel_initializer = kernel_initializer,
                             gated_activation = gated_activation,
                             kernel_dilated_conv = kernel_dilated_conv,
                             residual_input = None,
                             batch_normalization = batch_normalization,
                             bn_epsilon = bn_epsilon)

  # -------------------- #
  # Layer 3: dilation 4
  # -------------------- #
  wavenet_no_padding_layer_3_skip, wavenet_no_padding_layer_3_output  = wavenet_layer_no_padding(input = wavenet_no_padding_layer_2_output,
                             layer_number = 3, dilation = 4,
                             channels_dilated_conv = channels_dilated_conv,
                             channels_1x1conv = channels_1x1conv,
                             kernel_regularizer_dilated_conv = kernel_regularizer_dilated_conv,
                             kernel_regularizer_1x1_conv = kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv = bias_regularizer_dilated_conv,
                             bias_regularizer_1x1_conv = bias_regularizer_1x1_conv,
                             kernel_initializer = kernel_initializer,
                             gated_activation = gated_activation,
                             kernel_dilated_conv = kernel_dilated_conv,
                             residual_input = None,
                             batch_normalization = batch_normalization,
                             bn_epsilon = bn_epsilon)

  # -------------------- #
  # Layer 4: dilation 8
  # -------------------- #
  wavenet_no_padding_layer_4_skip, wavenet_no_padding_layer_4_output  = wavenet_layer_no_padding(input = wavenet_no_padding_layer_3_output,
                             layer_number = 4, dilation = 8,
                             channels_dilated_conv = channels_dilated_conv,
                             channels_1x1conv = channels_1x1conv,
                             kernel_regularizer_dilated_conv = kernel_regularizer_dilated_conv,
                             kernel_regularizer_1x1_conv = kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv = bias_regularizer_dilated_conv,
                             bias_regularizer_1x1_conv = bias_regularizer_1x1_conv,
                             kernel_initializer = kernel_initializer,
                             gated_activation = gated_activation,
                             kernel_dilated_conv = kernel_dilated_conv,
                             residual_input = None,
                             batch_normalization = batch_normalization,
                             bn_epsilon = bn_epsilon)

  # -------------------- #
  # Layer 5: dilation 16
  # -------------------- #
  wavenet_no_padding_layer_5_skip, __  = wavenet_layer_no_padding(input = wavenet_no_padding_layer_4_output,
                             layer_number = 5, dilation = 16,
                             channels_dilated_conv = channels_dilated_conv,
                             channels_1x1conv = channels_1x1conv,
                             kernel_regularizer_dilated_conv = kernel_regularizer_dilated_conv,
                             kernel_regularizer_1x1_conv = kernel_regularizer_1x1_conv,
                             bias_regularizer_dilated_conv = bias_regularizer_dilated_conv,
                             bias_regularizer_1x1_conv = bias_regularizer_1x1_conv,
                             kernel_initializer = kernel_initializer,
                             gated_activation = gated_activation,
                             kernel_dilated_conv = kernel_dilated_conv,
                             residual_input = None,
                             batch_normalization = batch_normalization,
                             bn_epsilon = bn_epsilon)

  # -------------------- #
  # Skip concatenation
  # -------------------- #

  # Last skip
  ## channel-distributed (channel-dist shared dense)
  decoder_ch_dist = tf.transpose(wavenet_no_padding_layer_5_skip, perm=[0, 2, 1])
  decoder_ch_dist_wrap = layers.Dense(output_neurons, activation = None,
                            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                            kernel_initializer = kernel_initializer)
  decoder_ch_dist = layers.TimeDistributed(decoder_ch_dist_wrap)(decoder_ch_dist)
  decoder_ch_dist = tf.transpose(decoder_ch_dist, perm=[0, 2, 1])


  ### Previous Skip connections
  ## channel-distributed (channel-wise shared dense)
  skip_1 = tf.transpose(wavenet_no_padding_layer_1_skip, perm=[0, 2, 1])
  ch_dist_skip_1_wrap = layers.Dense(output_neurons, activation = None,
                            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                            kernel_initializer = kernel_initializer)
  skip_1 = layers.TimeDistributed(ch_dist_skip_1_wrap)(skip_1)
  skip_1 = tf.transpose(skip_1, perm=[0, 2, 1])

  skip_2 = tf.transpose(wavenet_no_padding_layer_2_skip, perm=[0, 2, 1])
  ch_dist_skip_2_wrap = layers.Dense(output_neurons, activation = None,
                            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                            kernel_initializer = kernel_initializer)
  skip_2 = layers.TimeDistributed(ch_dist_skip_2_wrap)(skip_2)
  skip_2 = tf.transpose(skip_2, perm=[0, 2, 1])

  skip_3 = tf.transpose(wavenet_no_padding_layer_3_skip, perm=[0, 2, 1])
  ch_dist_skip_3_wrap = layers.Dense(output_neurons, activation = None,
                            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                            kernel_initializer = kernel_initializer)
  skip_3 = layers.TimeDistributed(ch_dist_skip_3_wrap)(skip_3)
  skip_3 = tf.transpose(skip_3, perm=[0, 2, 1])

  skip_4 = tf.transpose(wavenet_no_padding_layer_4_skip, perm=[0, 2, 1])
  ch_dist_skip_4_wrap = layers.Dense(output_neurons, activation = None,
                            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                            kernel_initializer = kernel_initializer)
  skip_4 = layers.TimeDistributed(ch_dist_skip_4_wrap)(skip_4)
  skip_4 = tf.transpose(skip_4, perm=[0, 2, 1])

  skip_concatenation = keras.layers.Concatenate(axis=-1)([skip_1,skip_2, skip_3,
                                                                      skip_4, decoder_ch_dist])

  if (batch_normalization is True):
      skip_concatenation = batch_norm()(skip_concatenation)

  skip_concatenation = tf.keras.layers.LeakyReLU()(skip_concatenation)

  if (dropout is not None and perc_dropout_unpwavenet > 0):
      skip_concatenation = doprout_custom(perc_dropout_unpwavenet)(skip_concatenation)

  # 1x1 equivalent to apply a TimeDistributed dense layer
  decoder_last_1x1 = layers.Conv1D(channels_1x1conv, kernel_size = 1, activation = None,
                                      kernel_regularizer = kernel_regularizer_1x1_conv, bias_regularizer=bias_regularizer_1x1_conv,
                                      kernel_initializer=kernel_initializer)(skip_concatenation)

  if (batch_normalization is True):
      decoder_last_1x1 = batch_norm()(decoder_last_1x1)

  decoder_last_1x1 = tf.keras.layers.LeakyReLU()(decoder_last_1x1)

  decoder_last_1x1 = layers.Flatten()(decoder_last_1x1)

  decoder_output = layers.Dense(1, activation = "linear",kernel_regularizer=kernel_regularizer,
                                              bias_regularizer=bias_regularizer,
                                              kernel_initializer=kernel_initializer)(decoder_last_1x1)

  meteo_model_unpwavenet = keras.models.Model(model_input, decoder_output)

  return meteo_model_unpwavenet

### LSTM ###########

def meteo_model_time_dist_dilated_lstm(meteo_input = None,
                                       input_month_encodings = False,
                                       past_meteorological_images = 24,
                                       output_neurons = None,
                                       lstm_units = 32,
                                       dropout = None,
                                       perc_dropout = 0.4,
                                       lstm_activation = "tanh",
                                       kernel_regularizer = None,
                                       kernel_regularizer_lstm = None,
                                       bias_regularizer = None,
                                       bias_regularizer_lstm = None,
                                       recurrent_regularizer_lstm=None,
                                       kernel_initializer = "HeUniform",
                                       batch_normalization = False,
                                       bn_epsilon = 0.001):

  """
  Initialization of the TDC-LSTM architecture

  --- parameters ---
  "meteo_input": array-like KerasTensor, optional, input data
  "output_neurons": int, number of output neurons. Current version implemented for the seq2val case (output_neurons = 1)
  "lstm_units": int, number of units in each LSTM cell
  "lstm_activation": str, activation to be used in the gates (values) of the LSTM layer
  "past_meteorological_images": int, time-length of the input image time series
  "input_month_encodings": whether to use as input also the OHE of each time step of the input image time series
  "dropout": str, optional, whether to use dropout between the TDC and the UnPWaveNet. it could be "classic" or "mc" for the monte carlo dropout
  "perc_dropout": float, dropout probability
  "kernel_regularizer": tf.keras.regularizers, kernel regularizer for the TDC module
  "kernel_regularizer_lstm": tf.keras.regularizers, kernel regularizer for the LSTM layer
  "bias_regularizer": tf.keras.regularizers, bias regularizer for the TDC module
  "bias_regularizer_lstm: tf.keras.regularizers, bias regularizer for the LSTM layer
  "recurrent_regularizer_lstm": tf.keras.regularizers, kernel regularizer for recurrent input LSTM layer
  "kernel_initializer": str, initializer for kernel weights
  "batch_normalization": bool, wether to use Batch Normalization, if yes it is placed after convolution and before activation as in ResNet
  "bn_epsilon": float, the epsilon parameter to be used in the BN

  --- return ---
  a Keras.Model

  """

  if(meteo_input is None):
      # In a KerasTensor is not provided
      meteo_input = layers.Input(shape=(past_meteorological_images,5,8,3))


  if (dropout == "mc"):
    # MC dropout: drop either in training and inference
    class doprout_custom(tf.keras.layers.SpatialDropout1D):
      def call(self, inputs, training):
        return super().call(inputs, training=True)

  if (dropout == "classic"):
    class doprout_custom(tf.keras.layers.SpatialDropout1D):
      def call(self, inputs, training):
        return super().call(inputs, training)

  meteo_flattened = meteo_time_distributed_conv_module(meteo_input,
                                                       kernel_regularizer = kernel_regularizer,
                                                       bias_regularizer = bias_regularizer,
                                                       batch_normalization = batch_normalization,
                                                       bn_epsilon = bn_epsilon,
                                                       kernel_initializer = kernel_initializer)



  if (batch_normalization is True):
      batch_norm = partial(tf.keras.layers.BatchNormalization,
                  epsilon=bn_epsilon)

  if (input_month_encodings is True):
      month_encodings = layers.Input(shape=(past_meteorological_images,11))
      model_input = [meteo_input, month_encodings]

      meteo_flattened = keras.layers.Concatenate(axis=-1)([meteo_flattened, month_encodings])

  else:
      model_input = meteo_input


  input_bottleneck_wrap = layers.Dense(16, activation = None,
                       kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,
                       kernel_initializer = kernel_initializer)
  input_bottleneck = layers.TimeDistributed(input_bottleneck_wrap)(meteo_flattened)

  if (batch_normalization is True):
      input_bottleneck = batch_norm()(input_bottleneck)

  input_bottleneck = tf.keras.layers.LeakyReLU()(input_bottleneck)


  if (dropout is not None):
      # apply a dropout mask this should help the network to use all the channels and
      # avoid that one channel overifits
      input_bottleneck = doprout_custom(perc_dropout)(input_bottleneck)


###### LSTM ########################################


  decoder_lstm = keras.layers.LSTM(lstm_units, activation=lstm_activation, recurrent_activation="sigmoid",
                          kernel_regularizer=kernel_regularizer_lstm,
                          recurrent_regularizer=recurrent_regularizer_lstm,
                          bias_regularizer=bias_regularizer_lstm,
                          kernel_initializer = kernel_initializer,
                          return_sequences=False, return_state=False)(input_bottleneck)

  decoder_lstm = layers.Dense(8, activation = "leaky_relu",kernel_regularizer=kernel_regularizer,
                                  bias_regularizer=bias_regularizer,
                                  kernel_initializer = kernel_initializer)(decoder_lstm)

  decoder_output = layers.Dense(1, activation = "linear",kernel_regularizer=kernel_regularizer,
                                  bias_regularizer=bias_regularizer,
                                  kernel_initializer = kernel_initializer)(decoder_lstm)


  meteo_model_lstm = keras.models.Model(model_input, decoder_output)

  return meteo_model_lstm