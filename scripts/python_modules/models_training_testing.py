# -*- coding: utf-8 -*-
"""models_training_testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gI_4P79OLfuuoUoHS39INgp76Vc848zK
"""

import numpy as np
import matplotlib.pyplot as plt
import xarray
import pandas as pd
import copy
import datetime
from datetime import datetime, timedelta
import seaborn as sns
import geopandas as gpd
import datetime
import os
from itertools import compress
import random
import re
import math
import scipy.stats

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import activations
import sys
import warnings
from tensorflow.keras.callbacks import TensorBoard
import pickle

"""# Model settings and training"""

def setting_model(model, models_dir, optimizer,
                  new_folder_name, initial_epoch = 0, logdir = None,
                  loss = 'mean_squared_error', metrics = ['mean_squared_error'],
                  model_name = '', history_name = ''):

  """
  Compile model, create directory using current date-time, and paste it in front of names
  if the logdir is provided it is assumed that a model already axist and one want to continue the training

  --- parameters ---
  "model": Keras Model, model to be compiled
  "models_dir": str, main folder in which create the logs subfolder
  "optimizer": Keras Optimizer, to be used for training the model
  "new_folder_name": str, name to be pasted after current date-time to the logs subfolder name
  "initial_epoch": int, epoch from which to start the training useful if one want to resume a training
  "logdir": str, optional, if provided it is assumed that a model already exist and one want to continue the training then no subfolder will be created
  "loss": str, Keras loss to be used in the training
  "metrics": list of str, Keras metrics to be computed during the training
  "model_name": str, name to be used for model saving files
  "history_name": str, name to be used for model history saving file

  --- return ---
  tuple containing directory and file names: logs directory, model, weights, and history names
  """

  if (logdir == None):
    model.compile(loss=loss, optimizer = optimizer, metrics = metrics)
    time_now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    logdir = os.path.join(models_dir, time_now +"_"+new_folder_name+"/")

  else:
    if (initial_epoch == 0):
      warnings.warn("Initial epoch set to 0! Previous epochs will be forgotten by Tensorboard...\n")


  model_file = time_now + '_' + model_name + '.keras'
  wights_file = time_now + '_' + model_name + '.weights.h5'
  history_file = time_now + '_' + history_name + '.history.npy'

  filename_model = logdir + model_file
  filename_weights = logdir + wights_file
  filename_hist = logdir + history_file

  return logdir, filename_model, filename_weights, filename_hist



def train_model(model, x_train, y_train, x_val, y_val, epochs, batch_size,
                initial_epoch, filename_model, filename_weights, filename_hist, tensorboard_callback,
                save = True,  save_freq = 4600, shuffle = True):
  """
  Train a Keras model

  --- parameters ---
  "model": Keras Model, compiled model to be trained
  "x_train": array-like, training input object
  "y_train": array-like, training output object
  "x_val": array-like, validation input object
  "y_val": array-like, validation output object
  "epochs": int, total number of epochs
  "batch_size": int, size of batches
  "initial_epoch": int, epoch from which to start the training
  "filename_model": str, path for saving model (.keras)
  "filename_weights": str, path for saving weights (.h5)
  "filename_hist": str, path for saving history (.npy)
  "tensorboard_callback": TensorBoard callback, to visualize the tensorboard during the training
  "save": bool, if True at the end of the training model, weights, and history will be saved
  "save_freq": int, number of iteration after which save a checkpoint
  "shuffle": bool, whether to shuffle the dataset before creating batches et every epoch

  --- return ---
  tf.keras.callbacks.History object

  """

  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=filename_model,
    verbose= 1,
    save_weights_only=False,
    save_freq=save_freq,
    save_best_only=False)


  history = model.fit(
    x_train,
    y_train,
    initial_epoch = initial_epoch,
    batch_size=batch_size,
    epochs=epochs,
    shuffle = shuffle,
    validation_data=(x_val, y_val),
    callbacks=[tensorboard_callback, model_checkpoint_callback])

  if (save == True):

    model.save(filename_model, overwrite = True)
    model.save_weights(filename_weights, overwrite=True)
    np.save(filename_hist, history.history)

  return history

"""# Evaluation metrics"""

def compute_RMSE(predictions, truth, axis = None):
  """
  Compute RMSE between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing RMSE at each time step of the output sequence

  ---return---
  array-like
  """
  rmse = np.sqrt(np.mean(np.square(predictions - truth), axis = axis))
  return rmse

def compute_RRMSE(predictions, truth, mean_truth = None, axis = None):
  """
  Compute RRMSE between true values and predicted ones. Equal to 1-NSE
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "mean_truth": float, optional, mean to be used for scaling the error if not provided it will be computed over the true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing RRMSE at each time step of the output sequence

  ---return---
  array-like
  """

  if (mean_truth == None):
    mean_truth = truth.mean(axis = axis)

  numerator = compute_RMSE(predictions, truth, axis = axis)
  denominator = compute_RMSE(truth, mean_truth, axis = axis)
  RRMSE = numerator/denominator
  return RRMSE

def compute_NRMSE(predictions, truth, normalization = "range", denominator = None,
                  axis = None):
  """
  Compute NRMSE between true values and predicted ones. Sometimes named rRMSE
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "normalization": str, type of scaling (normalization) to be applied it could be "range" or "mean"
  "denominator": float, optional, normalization factor to be used for scaling the error if not provided and "normalization" is specified it will be computed over the true values.
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing NRMSE at each time step of the output sequence

  ---return---
  array-like
  """
  if ((normalization == "range") and (denominator == None)):
    min = truth.min(axis = axis)
    max = truth.max(axis = axis)
    denominator = max - min
  if ((normalization == "mean") and (denominator == None)):
    denominator = truth.mean()

  NRMSE = compute_RMSE(predictions, truth, axis = axis)/denominator
  return NRMSE

def compute_BIAS(predictions, truth, axis = None):
  """
  Compute BIAS between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing BIAS at each time step of the output sequence

  ---return---
  array-like
  """
  bias = np.mean(predictions - truth, axis = axis)
  return bias

def compute_NBIAS(predictions, truth, max_value = None, min_value = None, axis = None):
  """
  Compute NBIAS between true values and predicted ones normalizing for the range of true values
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "max_value": float, optional, maximum used for normalizing if not provided is computed on the true values
  "min_value": float, optional, minimum used for normalizing if not provided is computed on the true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing NBIAS at each time step of the output sequence

  ---return---
  array-like
  """

  if ((min_value == None) and (max_value == None)):
    min_value = truth.min(axis = axis)
    max_value = truth.max(axis = axis)
  nbias = np.mean((predictions - truth)/(max_value - min_value), axis = axis)
  return nbias

def compute_ape(predictions, truth):
  """
  Compute APE between true values and predicted ones per each instance and, in seq2seq for each time step of the output sequence
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  ---return---
  array-like
  """
  ape = abs(predictions - truth)/truth
  return ape

def compute_mape(predictions, truth, axis = None):
  """
  Compute MAPE between true values and predicted ones . It gives heavier penalty on negative errors
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing MAPE at each time step of the output sequence

  ---return---
  array-like
  """
  mape = np.mean(compute_ape(predictions, truth), axis = axis)
  return mape

def compute_nash_sutcliffe(predictions, truth, truth_mean = None,
                           axis = None):
  """
  Compute NSE between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "truth_mean": float, optional, mean to be used for scaling the error if not provided it will be computed over the true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing NSE at each time step of the output sequence

  ---return---
  array-like
  """

  if (truth_mean == None):
    truth_mean = truth.mean(axis = axis)

  numerator = np.sum(np.square(predictions - truth), axis = axis)
  denominator = np.sum(np.square(truth_mean - truth), axis = axis)
  nse = 1-(numerator/denominator)
  return nse

def compute_legates_mccabe_eff(predictions, truth, truth_mean = None,
                               axis = None):
  """
  Compute Legates and McCabe Efficiency (1999) between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "truth_mean": float, optional, mean to be used for scaling the error if not provided it will be computed over the true values
  "axis": int, optional, the axis on which compute the mean. If None it return a scalare computing the average over all the istances. For a seq2seq scenario could be setted to 0 for computing LMC at each time step of the output sequence

  ---return---
  array-like
  """

  if (truth_mean == None):
    truth_mean = truth.mean(axis = axis)

  numerator = np.sum(abs(truth - predictions), axis = axis)
  denominator = np.sum(abs(truth - truth_mean), axis = axis)
  lmc = 1-(numerator/denominator)
  return lmc

def compute_kge(predictions, truth, truth_mean = None, truth_sd = None):

  """
  Compute KGE between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values
  "truth_mean": float, optional, mean to be used for scaling the error if not provided it will be computed over the true values
  "truth_sd": float, optional, standard deviation to be used for scaling the error if not provided it will be computed over the true values

  ---return---
  array-like
  """

  if (truth_mean == None):
    truth_mean = truth.mean()

  if (truth_sd == None):
    truth_sd = truth.std(ddof = 0)

  r = scipy.stats.pearsonr(predictions, truth)[0]  # Pearson's correlation coeff
  alfa = (np.std(predictions))/(truth_sd) # pred std / observed std
  beta = (np.mean(predictions))/(truth_mean) # pred mean / observed mean
  summation = (np.square(r-1)+np.square(alfa-1)+np.square(beta-1))
  kge = 1-np.sqrt(summation)
  return kge

def compute_rcorrelation_coeff(predictions, truth):
  """
  Compute rho between true values and predicted ones
  ---parameters---
  "predictions": array-like, predicted values
  "truth": array-like, true values

  ---return---
  float
  """
  r = scipy.stats.pearsonr(predictions, truth)[0]  # Pearson's correlation coeff
  return r

"""# Loading ensemble members"""

def load_models(models_list, path, model_ID, load_member = None, load = "model"):

  """
  Function to load trained ensemble members into a list

  ---parameters---
  "models_list": list, list length equal to the total number of members
  "path": str, main logs directory in which all the trained ensemble member models are stored
  "model_ID": str, model identifier either "UnPWaveNet" o "LSTM"
  "load_member": int, optional, to load a specific ensemble member
  "load": str, defines what to load either entire model ("model") or training histories ("history")

  ---return---
  list of Keras Model if "load" == "model" or list of keras history object if "load" == "history"
  """

  items_list = os.listdir(path)

  model_folder_list = [item for item in items_list if model_ID in item]
  model_folder_list.sort(key = lambda model : int(model[-2:].replace("_",""))) # sort in increasing order the models

  if load_member is not None:
    # load specific member
    model_folder_list = [item for item in model_folder_list if f'_{load_member}' in item]



  if (load == "history"):
    # load the histories of members
    hisotry = []
    model_history_complete_path =  [path + "/" + model_folder_list[i]+ "/" + model_folder_list[i] + ".history.npy" for i in range(0,len(model_folder_list))]
    print("\nLoading paths: \n")
    print(*model_history_complete_path[:len(models_list)], sep = "\n")
    for i in range(len(models_list)):
      hisotry.append(np.load(model_history_complete_path[i], allow_pickle='TRUE').item())
    return hisotry


  if (load == "model"):
    # load the trained models of memebers
    class doprout_custom(tf.keras.layers.SpatialDropout1D):
      def call(self, inputs, training):
        return super().call(inputs, training)

    loaded_models_list = []
    model_complete_path =  [path + "/" + model_folder_list[i]+ "/" + model_folder_list[i] + ".keras" for i in range(0,len(model_folder_list))]
    print("\nLoading paths: \n")
    print(*model_complete_path[:len(models_list)], sep = "\n")
    for i in range(0,len(models_list)):
      loaded_models_list.append(keras.models.load_model(model_complete_path[i],
                        custom_objects={"doprout_custom": doprout_custom}))
    return loaded_models_list